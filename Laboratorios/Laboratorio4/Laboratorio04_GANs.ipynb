{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d40anh2Y3Q_H"
      },
      "source": [
        "# **Laboratorio 04  SIS421**\n",
        "\n",
        "### Nombre: Quispe Sucullani Jose David\n",
        "\n",
        "### C.U. 111-376\n",
        "\n",
        "### Link github:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIJnb63G3kwF"
      },
      "source": [
        "##Contexto del Laboratorio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "sCaNiEq7J_CP",
        "outputId": "99e77fd6-3e98-4a19-f79a-e29f113dd075"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import os\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmhQ7-wuHuyh"
      },
      "source": [
        "##1: Lectura del Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvQ35sjHHsHo",
        "outputId": "70c121e7-4af9-4927-949b-84b77bfdbe9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de imágenes de entrenamiento: 60000\n",
            "Número de imágenes de prueba: 10000\n"
          ]
        }
      ],
      "source": [
        "# Definir las transformaciones para las imágenes\n",
        "from torchvision import datasets, transforms\n",
        "import torch.utils.data\n",
        "\n",
        "# Definir las transformaciones para el conjunto de datos FashionMNIST\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((28, 28)),  # Redimensionar las imágenes a 28x28 píxeles para FashionMNIST\n",
        "    transforms.ToTensor(),  # Convertir las imágenes a tensores\n",
        "    transforms.Normalize([0.5], [0.5])  # Normalizar entre [-1, 1] para las imágenes en escala de grises\n",
        "])\n",
        "\n",
        "# Cargar el conjunto de datos de entrenamiento y prueba de FashionMNIST con las transformaciones\n",
        "train_data = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Mostrar la cantidad de imágenes de entrenamiento y prueba\n",
        "print(f\"Número de imágenes de entrenamiento: {len(train_data)}\")\n",
        "print(f\"Número de imágenes de prueba: {len(test_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "-qkHfbG1H2TB"
      },
      "outputs": [],
      "source": [
        "# Function to handle dataset limiting (not needed for FashionMNIST, so keeping it as is but it won't be used directly)\n",
        "def limit_dataset_size(root_dir, max_per_class=6000):\n",
        "    # Dictionary to store paths of selected images by class\n",
        "    limited_dataset = []\n",
        "\n",
        "    # Iterate through subfolders in root_dir (each subfolder is a class)\n",
        "    for class_name in os.listdir(root_dir):\n",
        "        class_dir = os.path.join(root_dir, class_name)\n",
        "        if os.path.isdir(class_dir):\n",
        "            # Get all images in the subfolder\n",
        "            all_images = os.listdir(class_dir)\n",
        "            # Filter to get only images, excluding other file types\n",
        "            all_images = [os.path.join(class_name, img) for img in all_images if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
        "            # Randomly select 6000 images\n",
        "            selected_images = random.sample(all_images, min(len(all_images), max_per_class))\n",
        "            limited_dataset.extend(selected_images)\n",
        "\n",
        "    return limited_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3Dd71InH5Ty",
        "outputId": "e6d8a7f3-3fe5-4710-a78c-169ae74e7b4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número total de imágenes en el nuevo conjunto de entrenamiento: 60000\n",
            "Distribución de imágenes por clase: Counter({9: 6000, 0: 6000, 3: 6000, 2: 6000, 7: 6000, 5: 6000, 1: 6000, 6: 6000, 4: 6000, 8: 6000})\n"
          ]
        }
      ],
      "source": [
        "# For FashionMNIST, we don't need a custom dataset class to limit size.\n",
        "# The train_data loaded in the previous cell is already the full dataset.\n",
        "\n",
        "# Verify the number of images in the training set (already done in the previous cell)\n",
        "print(f\"Número total de imágenes en el nuevo conjunto de entrenamiento: {len(train_data)}\")\n",
        "\n",
        "# Verify the number of images per class (FashionMNIST is balanced)\n",
        "from collections import Counter\n",
        "# Get the targets from the train_data object\n",
        "class_distribution = Counter(train_data.targets.tolist())\n",
        "print(\"Distribución de imágenes por clase:\", class_distribution)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWDkI2v7H9S5",
        "outputId": "beae5547-93e1-4c66-8571-8235adb9eed6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clases en el conjunto de entrenamiento: ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
            "Cantidad de clases: 10\n"
          ]
        }
      ],
      "source": [
        "# 1. Ver las etiquetas del dataset (clases):\n",
        "# Ver las classes del dataset (etiquetas) for FashionMNIST\n",
        "print(f\"Clases en el conjunto de entrenamiento: {train_data.classes}\")\n",
        "print(f\"Cantidad de clases: {len(train_data.classes)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPcWQPvjIF3Q",
        "outputId": "ae09b740-2a77-4dc4-dd7c-bf100f1b312b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número total de imágenes en el conjunto de entrenamiento: 60000\n",
            "Número de clases: 10\n",
            "Clase 'Ankle boot' tiene 6000 imágenes\n",
            "Clase 'T-shirt/top' tiene 6000 imágenes\n",
            "Clase 'Dress' tiene 6000 imágenes\n",
            "Clase 'Pullover' tiene 6000 imágenes\n",
            "Clase 'Sneaker' tiene 6000 imágenes\n",
            "Clase 'Sandal' tiene 6000 imágenes\n",
            "Clase 'Trouser' tiene 6000 imágenes\n",
            "Clase 'Shirt' tiene 6000 imágenes\n",
            "Clase 'Coat' tiene 6000 imágenes\n",
            "Clase 'Bag' tiene 6000 imágenes\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from collections import Counter\n",
        "\n",
        "# Verify the total number of images in the training set\n",
        "num_total_images = len(train_data)\n",
        "print(f\"Número total de imágenes en el conjunto de entrenamiento: {num_total_images}\")\n",
        "\n",
        "# Verify the number of classes (labels) for FashionMNIST\n",
        "num_classes = len(train_data.classes)\n",
        "print(f\"Número de clases: {num_classes}\")\n",
        "\n",
        "# Get the labels of all images (train_data.targets has the labels for each image)\n",
        "class_distribution = Counter(train_data.targets.tolist())\n",
        "\n",
        "# Show how many images are in each class\n",
        "for class_idx, count in class_distribution.items():\n",
        "    print(f\"Clase '{train_data.classes[class_idx]}' tiene {count} imágenes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "5lp4KxDpL0xD",
        "outputId": "0898278d-ff37-40bd-e1e5-9c07cccdf74d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 10 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get the classes from the dataset\n",
        "classes = train_data.classes\n",
        "\n",
        "# Convert images and labels to numpy arrays for easier processing\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # Desnormalize\n",
        "    npimg = img.numpy()\n",
        "    # FashionMNIST is grayscale, so we handle it differently\n",
        "    if npimg.shape[0] == 1:\n",
        "        npimg = np.transpose(npimg, (1, 2, 0))[:, :, 0] # Remove the single channel dimension\n",
        "    else:\n",
        "        npimg = np.transpose(npimg, (1, 2, 0))\n",
        "    return npimg\n",
        "\n",
        "# Show 10 random images\n",
        "plt.figure(figsize=(10, 5))\n",
        "for i in range(10):\n",
        "    plt.subplot(2, 5, i + 1)\n",
        "    index = random.randint(0, len(train_data) - 1)\n",
        "    img, label = train_data[index]\n",
        "    plt.imshow(imshow(img), cmap='gray') # Use gray colormap for grayscale images\n",
        "    plt.title(classes[label])\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aspzm1KmttH"
      },
      "source": [
        "El dataset está formado por 6000 imágenes de baja resolución (50 x 50 píxeles, con tres canales de colores) y contiene 10 tipos prentas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "mQbp3CFL1Hst"
      },
      "outputs": [],
      "source": [
        "# Define a custom class for the dataset that normalizes to [-1, 1] (already handled by transform)\n",
        "# We can just use the loaded train_data directly with the DataLoader\n",
        "class Prendas(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.imgs = torch.stack([img[0] for img in dataset])  # Load images as tensors\n",
        "        # Normalization to [-1, 1] is done in the transform\n",
        "        self.labels = torch.tensor([img[1] for img in dataset], dtype=torch.long)  # Image labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.imgs[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "IxG0WzM81KA2"
      },
      "outputs": [],
      "source": [
        "# We don't need to create a separate normalized dataset class for FashionMNIST\n",
        "# The normalization is done in the transform when loading the data.\n",
        "# We will use the original 'train_data' with the DataLoader.\n",
        "train_dataset = train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "bfM3xlPT1XLY"
      },
      "outputs": [],
      "source": [
        "# Split images into batches\n",
        "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH_Cv5e90qek"
      },
      "source": [
        "#**MODELO DCGANs**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up0WzC2nuost"
      },
      "source": [
        "2. Definir el Generador (Generator)\n",
        "Ahora, ajustamos el generador para manejar imágenes a color (3 canales) de tamaño 50x50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "GYhXoOGuus1M"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.input_size = 100  # Size of the random input vector (latent vector)\n",
        "\n",
        "        self.inp = nn.Sequential(\n",
        "            nn.Linear(self.input_size, 7*7*256),  # Transform the input vector to a 7x7x256 feature map for 28x28 output\n",
        "            nn.BatchNorm1d(7*7*256),  # Batch normalization for stable training\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.main = nn.Sequential(\n",
        "            # First deconvolutional layer: 7x7x256 -> 14x14x128\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Second deconvolutional layer: 14x14x128 -> 28x28x64\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "\n",
        "            # Third deconvolutional layer: 28x28x64 -> 28x28x1 (Grayscale)\n",
        "            nn.ConvTranspose2d(64, 1, 3, stride=1, padding=1, bias=False), # Adjusted kernel size and stride for 28x28 output\n",
        "            nn.Tanh()  # Normalize the output to a [-1, 1] range\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.inp(x)\n",
        "        x = x.view(-1, 256, 7, 7)  # Reshape for 7x7 input with 256 channels\n",
        "        x = self.main(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LR2UaGBF2N8l",
        "outputId": "438fdaa5-eacb-450e-8cdf-c9cd09731e83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "# Verify the output size\n",
        "generator = Generator()\n",
        "output = generator(torch.randn(64, 100))\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G3VfjtAu6p9"
      },
      "source": [
        "###3. Definir el Discriminador (Discriminator)\n",
        "Modificamos el discriminador para aceptar imágenes a color de tamaño 50x50:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "OYEn586UvCXV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            # First convolutional layer: 28x28x1 -> 14x14x64\n",
        "            nn.Conv2d(1, 64, 4, stride=2, padding=1, bias=False), # Input channels changed to 1\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Second convolutional layer: 14x14x64 -> 7x7x128\n",
        "            nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Third convolutional layer: 7x7x128 -> 4x4x256\n",
        "            nn.Conv2d(128, 256, 3, stride=2, padding=1, bias=False), # Adjusted kernel size\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # Fourth convolutional layer: 4x4x256 -> 2x2x512 (or similar depending on padding/stride)\n",
        "            # Let's adjust the last conv layer for 4x4 to 1x1 output\n",
        "            nn.Conv2d(256, 512, 4, stride=1, padding=0, bias=False), # Adjusted kernel size and padding for 1x1 output\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        )\n",
        "\n",
        "        self.out = nn.Sequential(\n",
        "            # Linear layer for classification\n",
        "            nn.Linear(512*1*1, 1),  # Flatten the output tensor from the convolutional layer (now 1x1x512)\n",
        "            nn.Sigmoid()  # Output between 0 and 1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.main(x)  # Pass the input through the convolutional layers\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.out(x)  # Pass through the final linear layer\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wu73_hh2wr0",
        "outputId": "a6450a50-74aa-4d3a-b5c3-a1c71abef09a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 1])\n"
          ]
        }
      ],
      "source": [
        "discriminator = Discriminator()\n",
        "# Adjust the input size to match FashionMNIST images (grayscale 28x28)\n",
        "output = discriminator(torch.randn(64, 1, 28, 28))\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5StKBi1vG81"
      },
      "source": [
        "###4. Definimos el Proceso de Entrenamiento\n",
        "Configuramos el bucle de entrenamiento, la función de pérdida y los optimizadores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "IJeF8KtLvKmU"
      },
      "outputs": [],
      "source": [
        "from fastprogress import master_bar, progress_bar\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def fit(g, d, dataloader, epochs=100, crit=None, checkpoint_dir='/content/drive/MyDrive/SIS421-IA2/Laboratorios/LAB-04/', save_interval=5):\n",
        "    g.to(device)\n",
        "    d.to(device)\n",
        "\n",
        "    g_optimizer = torch.optim.Adam(g.parameters(), lr=3e-4)\n",
        "    d_optimizer = torch.optim.Adam(d.parameters(), lr=3e-4)\n",
        "\n",
        "    crit = nn.BCEWithLogitsLoss() if crit is None else crit\n",
        "\n",
        "    g_loss, d_loss = [], []\n",
        "    mb = master_bar(range(1, epochs+1))\n",
        "    hist = {'g_loss': [], 'd_loss': []}\n",
        "\n",
        "    # Create the checkpoint directory if it doesn't exist\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    for epoch in mb:\n",
        "        for X, _ in progress_bar(dataloader, parent=mb):\n",
        "            X = X.to(device)  # Ensure real images go to the device\n",
        "\n",
        "            # Train the discriminator\n",
        "            g.eval()\n",
        "            d.train()\n",
        "\n",
        "            # Generate a batch of fake images\n",
        "            noise = torch.randn((X.size(0), g.input_size)).to(device)  # Noise vector\n",
        "            generated_images = g(noise)  # Generate fake images\n",
        "\n",
        "            # Concatenate real and generated images for the discriminator\n",
        "            d_input = torch.cat([generated_images, X], dim=0)\n",
        "            # Create labels: 0 for fake images and 1 for real images\n",
        "            d_gt = torch.cat([torch.zeros(X.size(0)), torch.ones(X.size(0))]).view(-1, 1).to(device)\n",
        "\n",
        "            # Optimize the discriminator\n",
        "            d_optimizer.zero_grad()\n",
        "            d_output = d(d_input)\n",
        "            d_l = crit(d_output, d_gt)\n",
        "            d_l.backward()\n",
        "            d_optimizer.step()\n",
        "            d_loss.append(d_l.item())\n",
        "\n",
        "            # Train the generator\n",
        "            g.train()\n",
        "            d.eval()\n",
        "\n",
        "            # Generate a new batch of fake images\n",
        "            noise = torch.randn((X.size(0), g.input_size)).to(device)\n",
        "            generated_images = g(noise)\n",
        "\n",
        "            # Pass the fake images through the discriminator\n",
        "            d_output = d(generated_images)\n",
        "            # Generator's goal: fool the discriminator, so we use \"real\" labels (1)\n",
        "            g_gt = torch.ones(X.size(0)).view(-1, 1).to(device)\n",
        "\n",
        "            # Optimize the generator\n",
        "            g_optimizer.zero_grad()\n",
        "            g_l = crit(d_output, g_gt)\n",
        "            g_l.backward()\n",
        "            g_optimizer.step()\n",
        "            g_loss.append(g_l.item())\n",
        "\n",
        "            # Progress logs\n",
        "            mb.child.comment = f'g_loss {np.mean(g_loss):.5f} d_loss {np.mean(d_loss):.5f}'\n",
        "        mb.write(f'Epoch {epoch}/{epochs} g_loss {np.mean(g_loss):.5f} d_loss {np.mean(d_loss):.5f}')\n",
        "        hist['g_loss'].append(np.mean(g_loss))\n",
        "        hist['d_loss'].append(np.mean(d_loss))\n",
        "\n",
        "\n",
        "        # Save a checkpoint every 'save_interval' epochs\n",
        "        if epoch % save_interval == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'generator_state_dict': g.state_dict(),\n",
        "                'discriminator_state_dict': d.state_dict(),\n",
        "                'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
        "                'd_optimizer_state_dict': d_loss\n",
        "            }, os.path.join(checkpoint_dir, f'GANs_modelo_{epoch}.pth'))\n",
        "\n",
        "    return hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "IVntp3UGFnZc",
        "outputId": "db56796d-2c17-448d-c0ac-15303762f605"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<style>\n",
              "    /* Turns off some styling */\n",
              "    progress {\n",
              "        /* gets rid of default border in Firefox and Opera. */\n",
              "        border: none;\n",
              "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "        background-size: auto;\n",
              "    }\n",
              "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
              "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
              "    }\n",
              "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
              "        background: #F44336;\n",
              "    }\n",
              "</style>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      <progress value='18' class='' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      18.00% [18/100 17:20&lt;1:18:59]\n",
              "    </div>\n",
              "    \n",
              "Epoch 1/100 g_loss 9.59467 d_loss 0.00453<p>Epoch 2/100 g_loss 10.04815 d_loss 0.00271<p>Epoch 3/100 g_loss 10.62942 d_loss 0.00181<p>Epoch 4/100 g_loss 11.17569 d_loss 0.00136<p>Epoch 5/100 g_loss 11.46531 d_loss 0.00114<p>Epoch 6/100 g_loss 11.80786 d_loss 0.00095<p>Epoch 7/100 g_loss 12.18877 d_loss 0.00081<p>Epoch 8/100 g_loss 12.61874 d_loss 0.00071<p>Epoch 9/100 g_loss 13.05142 d_loss 0.00063<p>Epoch 10/100 g_loss 12.86121 d_loss 0.00088<p>Epoch 11/100 g_loss 12.75723 d_loss 0.00080<p>Epoch 12/100 g_loss 12.79532 d_loss 0.00073<p>Epoch 13/100 g_loss 12.88538 d_loss 0.00068<p>Epoch 14/100 g_loss 13.01057 d_loss 0.00063<p>Epoch 15/100 g_loss 13.20014 d_loss 0.00059<p>Epoch 16/100 g_loss 13.41866 d_loss 0.00055<p>Epoch 17/100 g_loss 13.66555 d_loss 0.00052<p>Epoch 18/100 g_loss 13.61193 d_loss 0.00093<p>\n",
              "\n",
              "    <div>\n",
              "      <progress value='1492' class='' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      79.57% [1492/1875 00:50&lt;00:12 g_loss 13.44068 d_loss 0.00089]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Ejecuta el entrenamiento\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "hist = fit(generator, discriminator, dataloader, crit=torch.nn.BCELoss())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCWeC1UDcNLt"
      },
      "source": [
        "## CONTINUANDO CON EL ENTRENAMIENTO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykN766VRcZvF"
      },
      "outputs": [],
      "source": [
        "from fastprogress import master_bar, progress_bar\n",
        "\n",
        "def load_checkpoint(checkpoint_path, g, d, g_optimizer, d_optimizer):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    g.load_state_dict(checkpoint['generator_state_dict'])\n",
        "    d.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "    g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
        "    d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1  # Reanudar desde el siguiente epoch\n",
        "    g_loss = checkpoint['g_loss']\n",
        "    d_loss = checkpoint['d_loss']\n",
        "    return start_epoch, g_loss, d_loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSKQXY9wcf3B"
      },
      "outputs": [],
      "source": [
        "from fastprogress import master_bar, progress_bar\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def load_checkpoint(checkpoint_path, g, d, g_optimizer, d_optimizer):\n",
        "    checkpoint = torch.load(checkpoint_path)\n",
        "    g.load_state_dict(checkpoint['generator_state_dict'])\n",
        "    d.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "    g_optimizer.load_state_dict(checkpoint['g_optimizer_state_dict'])\n",
        "    d_optimizer.load_state_dict(checkpoint['d_optimizer_state_dict'])\n",
        "    start_epoch = checkpoint['epoch'] + 1  # Resume from the next epoch\n",
        "    g_loss = checkpoint['g_loss']\n",
        "    d_loss = checkpoint['d_loss']\n",
        "    return start_epoch, g_loss, d_loss\n",
        "\n",
        "def fit(g, d, dataloader, epochs=100, crit=None, checkpoint_dir='/content/drive/MyDrive/SIS421-IA2/Laboratorios/LAB-04/', save_interval=5, resume_from_checkpoint=None):\n",
        "    g.to(device)\n",
        "    d.to(device)\n",
        "\n",
        "    g_optimizer = torch.optim.Adam(g.parameters(), lr=3e-4)\n",
        "    d_optimizer = torch.optim.Adam(d.parameters(), lr=3e-4)\n",
        "\n",
        "    crit = nn.BCEWithLogitsLoss() if crit is None else crit\n",
        "\n",
        "    # If resuming from a checkpoint, load the model, optimizers, and losses\n",
        "    if resume_from_checkpoint:\n",
        "        start_epoch, g_loss, d_loss = load_checkpoint(resume_from_checkpoint, g, d, g_optimizer, d_optimizer)\n",
        "    else:\n",
        "        start_epoch = 1\n",
        "        g_loss, d_loss = [], []\n",
        "\n",
        "    mb = master_bar(range(start_epoch, epochs+1))\n",
        "    hist = {'g_loss': [], 'd_loss': []}\n",
        "\n",
        "    # Create the checkpoint directory if it doesn't exist\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    for epoch in mb:\n",
        "        for X, _ in progress_bar(dataloader, parent=mb):\n",
        "            X = X.to(device)  # Ensure real images go to the device\n",
        "\n",
        "            # Train the discriminator\n",
        "            g.eval()\n",
        "            d.train()\n",
        "\n",
        "            # Generate a batch of fake images\n",
        "            noise = torch.randn((X.size(0), g.input_size)).to(device)  # Noise vector\n",
        "            generated_images = g(noise)  # Generate fake images\n",
        "\n",
        "            # Concatenate real and generated images for the discriminator\n",
        "            d_input = torch.cat([generated_images, X], dim=0)\n",
        "            # Create labels: 0 for fake images and 1 for real images\n",
        "            d_gt = torch.cat([torch.zeros(X.size(0)), torch.ones(X.size(0))]).view(-1, 1).to(device)\n",
        "\n",
        "            # Optimize the discriminator\n",
        "            d_optimizer.zero_grad()\n",
        "            d_output = d(d_input)\n",
        "            d_l = crit(d_output, d_gt)\n",
        "            d_l.backward()\n",
        "            d_optimizer.step()\n",
        "            d_loss.append(d_l.item())\n",
        "\n",
        "            # Train the generator\n",
        "            g.train()\n",
        "            d.eval()\n",
        "\n",
        "            # Generate a new batch of fake images\n",
        "            noise = torch.randn((X.size(0), g.input_size)).to(device)\n",
        "            generated_images = g(noise)\n",
        "\n",
        "            # Pass the fake images through the discriminator\n",
        "            d_output = d(generated_images)\n",
        "            # Generator's goal: fool the discriminator, so we use \"real\" labels (1)\n",
        "            g_gt = torch.ones(X.size(0)).view(-1, 1).to(device)\n",
        "\n",
        "            # Optimize the generator\n",
        "            g_optimizer.zero_grad()\n",
        "            g_l = crit(d_output, g_gt)\n",
        "            g_l.backward()\n",
        "            g_optimizer.step()\n",
        "            g_loss.append(g_l.item())\n",
        "\n",
        "            # Progress logs\n",
        "            mb.child.comment = f'g_loss {np.mean(g_loss):.5f} d_loss {np.mean(d_loss):.5f}'\n",
        "        mb.write(f'Epoch {epoch}/{epochs} g_loss {np.mean(g_loss):.5f} d_loss {np.mean(d_loss):.5f}')\n",
        "        hist['g_loss'].append(np.mean(g_loss))\n",
        "        hist['d_loss'].append(np.mean(d_loss))\n",
        "\n",
        "        # Save a checkpoint every 'save_interval' epochs\n",
        "        if epoch % save_interval == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'generator_state_dict': g.state_dict(),\n",
        "                'discriminator_state_dict': d.state_dict(),\n",
        "                'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
        "                'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
        "                'g_loss': g_loss,\n",
        "                'd_loss': d_loss\n",
        "            }, os.path.join(checkpoint_dir, f'GANs_modelo_{epoch}.pth'))\n",
        "\n",
        "    return hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K2ljZZpckpV"
      },
      "outputs": [],
      "source": [
        "# Ejecutar el entrenamiento desde el último checkpoint\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "checkpoint_path = '/content/drive/MyDrive/SIS421-IA2/Laboratorios/LAB-04/GANs_modelo_40.pth'  # Cambia esto por tu último checkpoint guardado\n",
        "hist = fit(generator, discriminator, dataloader, crit=torch.nn.BCELoss(), resume_from_checkpoint=checkpoint_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIZI_aFRENWt"
      },
      "outputs": [],
      "source": [
        "def fit(g, d, dataloader, epochs=100, crit=None, checkpoint_dir='/content/drive/MyDrive/SIS421-IA2/Laboratorios/LAB-04', save_interval=5, resume_from_checkpoint=None):\n",
        "    g.to(device)\n",
        "    d.to(device)\n",
        "\n",
        "    g_optimizer = torch.optim.Adam(g.parameters(), lr=3e-4)\n",
        "    d_optimizer = torch.optim.Adam(d.parameters(), lr=3e-4)\n",
        "\n",
        "    crit = nn.BCEWithLogitsLoss() if crit is None else crit\n",
        "\n",
        "    # Si se retoma desde un checkpoint, cargar el modelo, optimizadores y pérdidas\n",
        "    if resume_from_checkpoint:\n",
        "        start_epoch, g_loss, d_loss = load_checkpoint(resume_from_checkpoint, g, d, g_optimizer, d_optimizer)\n",
        "    else:\n",
        "        start_epoch = 1\n",
        "        g_loss, d_loss = [], []\n",
        "\n",
        "    mb = master_bar(range(start_epoch, epochs+1))\n",
        "    hist = {'g_loss': [], 'd_loss': []}\n",
        "\n",
        "    # Crear el directorio de checkpoints si no existe\n",
        "    if not os.path.exists(checkpoint_dir):\n",
        "        os.makedirs(checkpoint_dir)\n",
        "\n",
        "    for epoch in mb:\n",
        "        for X, _ in progress_bar(dataloader, parent=mb):\n",
        "            X = X.to(device)  # Aseguramos que las imágenes reales vayan al dispositivo\n",
        "\n",
        "            # Entrenamos el discriminador\n",
        "            g.eval()\n",
        "            d.train()\n",
        "\n",
        "            # Generamos un batch de imágenes falsas\n",
        "            noise = torch.randn((X.size(0), g.input_size)).to(device)  # Vector de ruido\n",
        "            generated_images = g(noise)  # Generamos imágenes falsas\n",
        "\n",
        "            # Concatenamos imágenes reales y generadas para el discriminador\n",
        "            d_input = torch.cat([generated_images, X], dim=0)\n",
        "            # Creamos etiquetas: 0 para imágenes falsas y 1 para reales\n",
        "            d_gt = torch.cat([torch.zeros(X.size(0)), torch.ones(X.size(0))]).view(-1, 1).to(device)\n",
        "\n",
        "            # Optimizamos el discriminador\n",
        "            d_optimizer.zero_grad()\n",
        "            d_output = d(d_input)\n",
        "            d_l = crit(d_output, d_gt)\n",
        "            d_l.backward()\n",
        "            d_optimizer.step()\n",
        "            d_loss.append(d_l.item())\n",
        "\n",
        "            # Entrenamos el generador\n",
        "            g.train()\n",
        "            d.eval()\n",
        "\n",
        "            # Generamos un nuevo batch de imágenes falsas\n",
        "            noise = torch.randn((X.size(0), g.input_size)).to(device)\n",
        "            generated_images = g(noise)\n",
        "\n",
        "            # Pasamos las imágenes falsas por el discriminador\n",
        "            d_output = d(generated_images)\n",
        "            # Objetivo del generador: engañar al discriminador, por eso usamos etiquetas de \"reales\" (1)\n",
        "            g_gt = torch.ones(X.size(0)).view(-1, 1).to(device)\n",
        "\n",
        "            # Optimizamos el generador\n",
        "            g_optimizer.zero_grad()\n",
        "            g_l = crit(d_output, g_gt)\n",
        "            g_l.backward()\n",
        "            g_optimizer.step()\n",
        "            g_loss.append(g_l.item())\n",
        "\n",
        "            # Logs de progreso\n",
        "            mb.child.comment = f'g_loss {np.mean(g_loss):.5f} d_loss {np.mean(d_loss):.5f}'\n",
        "        mb.write(f'Epoch {epoch}/{epochs} g_loss {np.mean(g_loss):.5f} d_loss {np.mean(d_loss):.5f}')\n",
        "        hist['g_loss'].append(np.mean(g_loss))\n",
        "        hist['d_loss'].append(np.mean(d_loss))\n",
        "\n",
        "        # Guardar un checkpoint cada 'save_interval' épocas\n",
        "        if epoch % save_interval == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'generator_state_dict': g.state_dict(),\n",
        "                'discriminator_state_dict': d.state_dict(),\n",
        "                'g_optimizer_state_dict': g_optimizer.state_dict(),\n",
        "                'd_optimizer_state_dict': d_optimizer.state_dict(),\n",
        "                'g_loss': g_loss,\n",
        "                'd_loss': d_loss\n",
        "            }, os.path.join(checkpoint_dir, f'GANs_modelo_{epoch}.pth'))\n",
        "\n",
        "    return hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYedGHsNEQkH"
      },
      "outputs": [],
      "source": [
        "# Ejecutar el entrenamiento desde el último checkpoint\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "checkpoint_path = '/content/drive/MyDrive/SIS421-IA2/Laboratorios/LAB-04/GANs_modelo_40.pth'  # Cambia esto por tu último checkpoint guardado\n",
        "hist = fit(generator, discriminator, dataloader, crit=torch.nn.BCELoss(), resume_from_checkpoint=checkpoint_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JES54RrJU8Vx"
      },
      "source": [
        "##EVALUACIÓN DE LOS MODELOS\n",
        "\n",
        "###Cargamos los modelos guardados para verificar el mejor medelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wtz7GKT9U7J7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "\n",
        "# Cargar un model dado\n",
        "def load_model(checkpoint_path, g, d, g_optimizer, d_optimizer):\n",
        "    mo1del = torch.load(checkpoint_path)\n",
        "    g.load_state_dict(mo1del['generator_state_dict'])\n",
        "    d.load_state_dict(mo1del['discriminator_state_dict'])\n",
        "    g_optimizer.load_state_dict(mo1del['g_optimizer_state_dict'])\n",
        "    d_optimizer.load_state_dict(mo1del['d_optimizer_state_dict'])\n",
        "    epoch = mo1del['epoch']\n",
        "    g_loss = mo1del['g_loss']\n",
        "    d_loss = mo1del['d_loss']\n",
        "    print(f\"Checkpoint loaded from epoch {epoch}\")\n",
        "    return epoch, g_loss, d_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gWleQc0AVfMT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "def generate_and_plot_images(g, num_images=5):\n",
        "    g.eval()  # Evaluation mode\n",
        "    noise = torch.randn(num_images, g.input_size).to(device)\n",
        "    with torch.no_grad():\n",
        "        generated_images = g(noise).cpu()\n",
        "\n",
        "    # Show the generated images\n",
        "    fig, axs = plt.subplots(1, num_images, figsize=(num_images * 3, 3))\n",
        "    for i in range(num_images):\n",
        "        img = generated_images[i].squeeze().permute(1, 0)  # Remove channel dimension for grayscale\n",
        "        img = (img + 1) / 2  # Desnormalize if using [-1, 1]\n",
        "        axs[i].imshow(img, cmap='gray') # Use gray colormap\n",
        "        axs[i].axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcFdqX3lW5zi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Definir el generador y discriminador\n",
        "generator = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "# Definir los optimizadores\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=3e-4, weight_decay=1e-4)\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=3e-4, weight_decay=1e-4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSvdKX6D4thB"
      },
      "source": [
        "Cargamos los modelos para ver los mejores valores resultados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCgfm8y_Vaea"
      },
      "outputs": [],
      "source": [
        "models = [\n",
        "    '/content/drive/MyDrive/SIS421-IA2/Laboratorios/LAB-04/GANs_modelo_30.pth',\n",
        "    '/content/drive/MyDrive/SIS421-IA2/Laboratorios/LAB-04/GANs_modelo_35.pth',\n",
        "    '/content/drive/MyDrive/SIS421-IA2/Laboratorios/LAB-04/GANs_modelo_40.pth'\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    epoch, g_loss, d_loss = load_model(model, generator, discriminator, g_optimizer, d_optimizer)\n",
        "    print(f\"Evaluando el checkpoint del epoch {epoch}: g_loss = {g_loss[-1]}, d_loss = {d_loss[-1]}\")\n",
        "    generate_and_plot_images(generator, num_images=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gj0oKS20ZKsP"
      },
      "outputs": [],
      "source": [
        "g_loss_total = []\n",
        "d_loss_total = []\n",
        "\n",
        "for model in models:\n",
        "    epoch, g_loss, d_loss = load_model(model, generator, discriminator, g_optimizer, d_optimizer)\n",
        "    g_loss_total.extend(g_loss)  # Agregar las pérdidas del generador\n",
        "    d_loss_total.extend(d_loss)  # Agregar las pérdidas del discriminador"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVK7_eRL453U"
      },
      "source": [
        "Graficamos los Loss tanto para Generador y Discriminador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHIXC7dlaW1h"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Crear un DataFrame con las pérdidas totales\n",
        "df = pd.DataFrame({\n",
        "    'g_loss': g_loss_total,\n",
        "    'd_loss': d_loss_total\n",
        "})\n",
        "\n",
        "# Graficar los `loss` del generador y discriminador\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(df['g_loss'], label='Generador Loss', color='blue')\n",
        "plt.plot(df['d_loss'], label='Discriminador Loss', color='red')\n",
        "plt.title('Evolución del Generador y Discriminador Loss')\n",
        "plt.xlabel('Iteraciones')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9mPlYplvaIE"
      },
      "source": [
        "###5. Visualización de los Resultados\n",
        "Al finalizar el entrenamiento, visualizamos las imágenes generadas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZGsExU7Nd6cm"
      },
      "outputs": [],
      "source": [
        "# Ruta del modelo que deseas cargar\n",
        "model_ = '/content/drive/MyDrive/SIS421-IA2/Laboratorios/LAB-04/GANs_modelo_35.pth'\n",
        "\n",
        "# Función para cargar el modelo\n",
        "epoch, g_loss, d_loss = load_model(model_, generator, discriminator, g_optimizer, d_optimizer)\n",
        "\n",
        "# Imprimir información del modelo cargado\n",
        "print(f\"Checkpoint del epoch {epoch} cargado: g_loss = {g_loss[-1]}, d_loss = {d_loss[-1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiWBC5_nvUMU"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "# Put the generator in evaluation mode\n",
        "generator.eval()\n",
        "\n",
        "# Generate images without calculating gradients\n",
        "with torch.no_grad():\n",
        "    # Create random noise to generate 10 images\n",
        "    noise = torch.randn((10, generator.input_size)).to(device)\n",
        "\n",
        "    # Generate images from the noise\n",
        "    generated_images = generator(noise)\n",
        "\n",
        "    # Configure the figure to display the images\n",
        "    fig, axs = plt.subplots(2, 5, figsize=(15, 5))\n",
        "    i = 0\n",
        "\n",
        "    # Iterate through each axis and display the images\n",
        "    for ax_row in axs:\n",
        "        for ax in ax_row:\n",
        "            # Reorganize image channels for visualization (from [C, H, W] to [H, W, C] - not needed for grayscale)\n",
        "            # Squeeze to remove the single channel dimension for grayscale images\n",
        "            img = generated_images[i].squeeze().cpu()\n",
        "\n",
        "            # Rescale image values from [-1, 1] to [0, 1] for visualization\n",
        "            ax.imshow((img + 1) / 2, cmap='gray')  # Use gray colormap\n",
        "            ax.axis('off')  # Hide axes\n",
        "            i += 1\n",
        "\n",
        "    # Show the generated images\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7xwSJISfyhw"
      },
      "source": [
        "En este caso las imágenes generadas son un poco mejores que las que obteníamos con la GAN simple, aunque todavía hay márgen de mejora."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}